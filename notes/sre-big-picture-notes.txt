

Site Reliability Engineering (SRE)

a way of managing production systems that solves the conflict between traditional dev and ops teams without all the cultural upheaval that we get with devops.

becoming increasingly popular because it provides a clear set of guildelines for implementation and result is an organization with reliable systems and high change velocity.


What is SRE?

	- an approach to operations which usses software as the primary tol for managing systems
	- sre has engineering capabailities which it puts to use into automating the management of the system or even improving the system itself.
	
	Eg. Organization has two teams: SRE and Product Development
		
		SRE Team:
		- administration tasks
			- automate as much as possible
		- own monitoring on systems & reporting on failures
		- responding to and dealing with these failures as they occur
		
		Development Team:
		- design
		- build
		- test
		
	Teams are jointly responsible for product, and can and will cross over.
	
	- If system is in poor state, and lots of support tickets, dev team can pickup issues
	- if system is in poor state because design isn't suitable, then SRE may work on design to build more scalable architecture
	
	- SRE's need a good understanding of the system being built
	- Dev's gain experience how the system works in production 
	
	- SRE similar to Ops, but the functions to execute are different
	
	- Eliminating Toil
		- Automating manual, repititive work
		- do this as much as possible with goal of improving the system overall
		- Toil is very low value work to be fixed, but often take up much of the time of the Ops teams.
		- Toil is often worked to patch up problems in the system and its a better use of time to fix the problem properly

	- Managing Risk - Working towards Service Levels
		- Agreed service levels with explicit tolerance
		- Risk Management is all about embracing change
		- encourage frequent, small releases from dev team which are less risky to deploy than big quarterly releases
		- Regular releases also support automated rollout and rollback processes. 
		- Changing systems brings risks even with automated process and SRE's addresses that explicitly using Service Level's with a built-in degree of tolerance and with a framework in place to reduce risk if the system goes out of tolerance

	- Handling Failure
		- Establish Incident management processes and post-mortems
		- makes reasonable use of SRE time
		- acknowledging of failures will happen
		- SRE views failure as an opportunity to improve the system
			- incidents always followed by blame free post-mortem to get to the root cause of the issues
			- put procedures or software tools in place to prevent failure from reoccuring
			
Background
	- SRE founding in Google when someone with Engineering background was run a Proudction/Operations team, and that is still how their sytems are run today
	- SRE predates DevOps movement	
		- How Google runs production systems
			- Engineering approach to ops, 2003
			- Site Reliability Engineering, 2016 **big-brain book
			- The Site Reliability Workbook, 2018 **followup big-brain book 
		
Comparing Traditional Ops and SRE
	Three Parties:
		"The Business" 
			- want the systems	
			- message to ops: 
				- i want my systems to be reliable or we'll lose business
			- message to devs:
				- i want new and exciting features in systems because if we don't keep improving we'll lose business
				- problem! -- making a new change wtihin system to introduce a new feature is when we are most to break it and reduce reliability
				- google's own internal research found 70% incidents in prod systems happen due to deployments of new software versions or configuration settings
		
		Ops Team 
			- run systems
			- reluctant to deploy as they know it might cause an outage
		
		Development Team 
			- build systems
			- want to deploy
			
	This uneasy tension between three parties puts a drag on the whole workflow/process
	
	Timeline for software lifecycle, either Agile or Waterfall:
		1 Design
		2 Development			Closer to Dev team: they own the first four aka build stage
		3 Testing 						
		4 Acceptance
							Once business accepts, the devs hand app to Ops team which is the only real interaction between the two.	
								
		5 Delivery				Closer to Ops team: they own the latter four, support, maintenance and capacity planning
		6 Support				
		7 Maintenance
		8 Capacity
		
	Clear delineation of responsibility here, which keeps teams apart despite certain area of delivery, where dev team hands over to ops, which is mostly the only form of real communication/collaboration.
	Either way, only invovles subset of each team to handle deployment, which is good for keeping tasks and roles seperate, but brings in a variety of problems.
	
	- Different goals
	- Different skillsets
	- Different tools
	- No common ground
	
	Ops Team:
	
		Black Box Delivery
			- No design input
			- Automation options limited
			- Infrasturcture-level monitoring
				- the team actually running the software have no input in how its dessigned or delivered until it happens
				- the architecture may not be suitable for the level of scale which the ops team needs to support
				- deliveray artifacts may not be suitable for automation:
					- if ops team wants to run within docker containers, it doesn't help if the dev team gives them windows installer files
				- because Ops hasn't been involved in the build stage, its common to see poor or useless high-level monitoring on these apps that make it to production
					- response latency, status codes
					- ops team becomes limited to basic monitoring of the infrasturcture level
					- only so much to be learned from CPU utilization and memory usage
					
		Limited Agency
			- Post-acceptance
			- GO/NO GO decision
			- Conflict guaranteed 

				- Ops have no say in this matter - it is decided that delivery will happen once the business accepts the work from the dev team.
				- Ops team have limited options when going live - may be blunt "choices" in a GO/NO GO decision
					- if they give a NO GO, this sets up conflict with dev team & business 
		
		After-the-event
			- Mandated service levels
			- Reactive support
				- Ops often given a delivery and told to run at a particular service level:
					- need to get into prod ASAP
					- keep it running!
					- if problems, every fix is reactive.
						- whole team can get swamped in process
						- looking at tickets, running a round, help me!!!
						- little chance to recover/improve systems to avoid repeat
						
	
	SRE Team: 
	
		1 Design *
		2 Development *			
		3 Testing 						
		4 Acceptance *
							
								
		5 Delivery*		
		6 Support*			
		7 Maintenance
		8 Capacity*
	
	- much more interaction between SRE + scrum team
		- Design
		- Development
		- Acceptance
		- Delivery
		- Support
		- Capacity
	
	- Dev team nominally own Build phase, but use SRE when they can
	- SRE nominally own the Run phase but use Dev team when they can.
	- Now, we get two teams that function as one team that has a lot more interaction between its members, that function as one, with shared goals and overlapping skillsets
		- Shared goal
		- Overlapping skillsets
			- SRE's have dev skills and devs are experiencing production
		- Consistent tools
			- core tool set used by both teams
		- Common basis
			- now have common basis for teams to work together.  Involving SRE as easy as build stage makes it much easier to run the system.
			
			
	SRE Team - 
	
		Agreed Delivery
			- Ongoing design input
			- Extended automation options
			- User-focused monitoring (useful reports)
				- Statistical monitoring vs cpu usage
				- can work with devs to implement that specific level of monitoring or add to the codebase themselves
		
		Full Agency - SRE has ultimate control over its own work
			- Dev team support
				- if prod support is taking too much time, devs come to help rather than work on new features
			- Stop deployments
				- if system has series of failures in given period, SRE team has power to stop any further releases to keep system stable
			- Hand back the pager
				- if system is not really production ready, SRE team can withdraw support, handing back the pager to dev team to mange the system themselves
		
		Key focus here is that even after the application is handed over from the devs, there is still cooperation, teamwork, interaction with both teams (SRE + Devs) 
			- handover to SRE might still mean most of team might work with the system after its been delivered, but they're doing that with an agreed service-level and aa formal limit to the amount of time fixing issues.
			


Comparing DevOps and SRE

    The Business
        Gimme reliability and new stuff
    DevOps
        "ok"

    Benefits of DevOps:
        the productivity of a small, capable, and empowered team can be amazing.

        The problem with DevOps is, it can be really hard to get there.

        Moving to DevOps is often done as a large, digital, transformation program which has extended periods where teams are transitioning to DevOps, so what the business gets is actually a lot of frustrating responses.

        Automation is needed to support regular, reliable releases.  However, projects with no automation require a lot of investment to build that out, and you can build new features while that is taking up time.

        Monitoring and feedback are core to DevOps as well, and older applications won't have any useful metrics.  Building that into the apps is crucial but it takes time away from delivering actual business value.


        If transformation process for a team to a genuine DevOps model is successful, then the DevOps team will have ownership all the major delivery functions, but they're owned by a single team.

            1 Design
		    2 Development
		    3 Testing
		    4 Acceptance
    		5 Delivery
	    	6 Support
		    7 Maintenance
		    8 Capacity

	    This team has all the same benefits of an SRE.
	        - One, shared goal
    	    - Overlapping skillsets
	        - Consistent tools
	        - Common basis/ground for everyone to work from

    	The positives are all the same, and the combined teams own all the functions.

	    However, keeping SRE separate helps keep things more aligned to the current implementation, and that makes the migration much easier for the enterprise.

    Overall, DevOps and SRE have much in Common:
        - Focus  on quality & velocity
        - Automation & tooling
        - Removing silos

    But DevOps...
        - has a broader remit
        - significant cultural change

    And SRE...
        - is more prescriptive
        - feasible internal migration


    Moving to DevOps is fundamentally about creating a single DevOps team from existing Development and Operations Teams.
        - Significant investment
        - Acceptance of change
        - Path can vary from company to company
        - Often needs outside help
        - Uneven rollout
            - some teams may gracefully accept, while other teams are more sluggish -- this can be normal, and due to varying reasons!

    Migrating to SRE removes a lot of those barriers, as SRE maintains the conceptual separation between DEV and OPS, even though Ops means something different in SRE, and Dev and SRE teams are more closer, Ops is still a separate function, and therefore is much easier to map onto the existing organization structure

    Another difficulty which DevOps struggles to address is the complexity of modern systems.

        A new micro-service application may be setup like this: written in Java Spring Boot, interacting with other microservices, an Angular UI, SQL JDBC Database, packaged in Docker, running on PCF, deployed to a Jenkins pipeline, Zuul to handle the network traffic, with BASH scripts to automate all the build + deployment steps, etc.

        Its a lot to ask for one team to be experts on all of these technologies, and therefore silo generation is commonplace.

        The team as a whole may have the expertise in all the technologies, not to mention the cloud services and data-service setup, but within the team the silos still exist.

   The picture is different with SRE.  Both teams don't need to have the same level of expertise along the same level of technology stack.

   Expertise can be split between the teams, allowing individuals to focus on their roles more effectively.



Exploring the Key Tenets of SRE

    Eliminating Toil
        - Toil is the sort of mundane, repetitive work that tends to pile up and usually falls to Ops; it doesn't add much value or improve the system.
            - The amount of work scales as the System scales

        - e.g. a Logging System which generates too many log entries and risks filling up the disk memory.
               - faced with this problem, an Ops team member may log into the System and clear / delete old log files.
               - boring, repetitive, necessary.
               - new release; logs fill disk daily
               - script to delete old log files; log in and run daily
               - scale up to 50 servers
               - CRON job to run script daily * possible end of Ops scenario

              *- SRE works with dev team to reduce logs

               The problem isn't really fixed -- No real improvement to the system and still potentially a lot of work here to manage.
               - now we have 50 CRON tasks to manage, and 50 copies of the same script to keep in sync

        SRE might take a similar approach, but final stage isn't just to be happy with multiple CRONs, its to go back to Dev team or codebase to change system to only send useful LOGs.

        - This improves the system as the LOG outputs only what we actually need and it successfully eliminates the toil.

    Working to + Maintaining Service Levels

        - eg. Given a system that is working well
            - new release; bugs cause latency
            - increase capacity; report issues to dev team
            - new release; config issue causes outage
            - fix config and restore service

           *- SRE can bloc knew releases for agreed period

            In Ops world, this is the end of the story.  We had some hiccups, two losses of service in short time frame, but we can blame them both on the Dev team, so its ok if we don't meet our target

            In SRE, that's different: Service Levels have an agreed upon level of tolerance over a given period.  If all that tolerance is used up, than an SRE can block releases for the rest of that given period.
            So we have the same amount of outage, but with no more releases we're much less likely to have any more outages for a given period.  So we have used up our tolerance, but we still meet our agreed upon Service Level.


    Managing Failure

        - e.g. DNS change causes outage
            - fix DNS issue

           *- SRE mandates a post-mortem into root cause

            In traditional Ops, that could be the end after applying the DNS config fix -- which, even though the System is up again, nothing has been learned from the failure.

            SRE mandates a post-mortem after every failure, which is a blameless process -- this aims at finding the root cause of the issue rather than finding out who to fire.

            In this case, it could be that the DNS configuration was being edited manually through a web-portal, which is always prone to being error-prone.
            The SRE approach would switch to using the API for the DNS provider, and build a tool which validates any new configuration before applying it.
                - The post-mortem and building the new tool take time, but they improve the System.  DNS issues shouldn't happen again, and the organization has a new, secure process for handling DNS changes, which other teams may also use.

        Adding engineering skills to the Ops team brings a very different approach to problems, which tends towards long-term improvements over short-term fixes.


Understanding Why SRE Works

    Accepts the conflict between Dev and Ops and addresses it explicitly.

    Conflict between dev and ops
        - made explicit
        - contracted service level
        - business buy-in
            - agree that a small degree of failure is acceptable
            - dev teams given room to deploy changes
            - SRE has the authority to stop deployments if quality dips

    Integration between dev and ops
        - Shared responsibility
        - Shared workload
        - Common toolkit
            - as Devs and SRE work more closely together, the lines between teams become blurred as collaboration increases.

    Prescriptive Practices
        - On-call teams and targets
        - %age SRE  time on toil
        - Service level framework

    IT-led Migration
        - Within existing org chart
        - Maintain product knowledge
        - Start small

    SRE Empowerment - SRE structure could feel more rewarding to Ops members
        - Improving systems
        - Formal structure
        - Tools to meet responsibilities
        - less finger-blaming between Ops and Devs, less stress within the work environment and easier problem solving

    SRE as an attractive role
        - Reduce mundane work
        - Expand skills
        - Career development


Module Summary and the Growth of an SRE

    "It's what a happens when you ask a software engineer to design an operations function."
        - Ben Treynor Sloss, VP Engineering, Founder of Google SRE
            - Michael Scott

    SRE is an approach to IT operations
    - Maintains a dev/ops split
    - But adds engineering to "ops"

    Shares goals with DevOps
    - Breaking down silos
    - increased quality and velocity

    Growing adoption
    - growing number of job roles and adoption across organizations
    - Sympathetic migration



Service Levels, Monitoring, and Alerting

	Understanding Service Level Objectives and Error Budgets
	
	Defining Service Level Indicators and Service Level Objectives

	Monitoring Servce Level Indicators
		Four Golden Signals

			Latency
				- Job processing time
				- Response generation time

			Traffic
				- Length of message queue
				- Requests per second

			Errors
				- Request failures
				- Response correctness

			Saturation
				- CPU & memory utilization
				- Network bandwidth
			
			These four signals will provide a good overall view of your system health, and if we can also track them for any service our apps depend on, then we have a great starting point for monitoring.

		Implementing SLIs

			With that said, our own SLIs are the real measure we need to capture because they'll drive alerts when we're faling to meet SLOs.  The more useful the data is, the more effort it will take.
			
			Example:
			Given a web application, we might have a user-focused SLO that says the home page should return in under half a second for over 95% of requests. There are lots of ways to write an SLI to measure this:
			Server logs:
				- Server Logs: scrape the logs that we already collect to report the response duration, or how long it takes the server to generate and respond to the request. However, this is not the same time period the user sees.  There will be load balanacing, CDNs, and the whole internet before they get a response from the server.
				
				- Synthetic Testing: External services, generate fake user requests from outside our infrastructure and records the end-to-end response times.  Pingdom/StatusCake are services that do this.  However, the synthetic response doesn't include rendering time in the browser, which could be significant if your home page loads data asynchronously from other services with Javascript.
				
				- Javascript Monitoring: include monitoring within Javascript, recording network load time, browser rendering.  However, this might need custom engineering effort.  Or, could use a service like Google Analytics, which records page latency together with traffic. 
				
			As it is shown in these examples, the choice of an SLI implementation is going to be a balance between how good the data is and how expensive it is to collect the data.  If starting from nothing, best guidance is to start simple and improve over time.
				
			
